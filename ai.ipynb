{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jezza/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Softmax, Conv2D, Input, MaxPooling2D, Flatten, RandomContrast\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV, KFold, cross_validate\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nCREATE THE .NPZ FILE\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "CREATE THE .NPZ FILE\n",
    "\n",
    "'''\n",
    "# folder_path = './data/MixedTrainingNumpy/'\n",
    "\n",
    "# data = {}\n",
    "\n",
    "# for file in os.listdir(folder_path):\n",
    "#     if file.endswith('.npy'):\n",
    "\n",
    "#         file_path = os.path.join(folder_path, file)\n",
    "#         array = np.load(file_path)\n",
    "\n",
    "#         label = file[0]\n",
    "\n",
    "#         data[label] = array\n",
    "\n",
    "# np.savez('./data/Training.npz', **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "LOADS AND \"FORMALISES\" DATA SO CAN BE PASSED INTO PREPROCESSING\n",
    "\n",
    "'''\n",
    "\n",
    "def loadData(filePath):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(filePath):\n",
    "        if filename.endswith('.npy'):\n",
    "\n",
    "            img = np.load(os.path.join(filePath, filename))\n",
    "\n",
    "            img = resize(img, (128, 128), anti_aliasing=True) # TODO - NOT NORMALISING DIDN'T YIELDED THE SAME VALUES\n",
    "\n",
    "            # display_img = np.clip(img * 255, 0, 255).astype('uint8')\n",
    "            # plt.imshow(display_img)\n",
    "            # plt.show()\n",
    "            \n",
    "            images.append(img)\n",
    "            \n",
    "            label = filename[0]\n",
    "            labels.append(label)\n",
    "\n",
    "    imagesNP = np.array(images)\n",
    "    labelsNP = np.array(labels)\n",
    "\n",
    "    # Adjust this if your labels are not numeric\n",
    "    encoder = LabelEncoder()\n",
    "    intLabels = encoder.fit_transform(labelsNP)\n",
    "    intLabels = to_categorical(intLabels)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    return imagesNP, intLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureExtractionModel():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(128,128,3))) # Images are 100 by 100 and RGB\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))#Break the image into separate sub-image\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Softmax Regression\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "    model.add(Dense(units=4, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filePath = './data/MixedTrainingNumpy/'\n",
    "    imagesNP, labelsNP = loadData(filePath)\n",
    "    print(\"Done\")\n",
    "\n",
    "    imTrain, imTest, labTrain, labTest = train_test_split(imagesNP, labelsNP, test_size=0.2, random_state=42)\n",
    "\n",
    "    # kFold = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    # foldN = 1\n",
    "    # bestLoss = 10\n",
    "\n",
    "    # for train, test in kFold.split(imTrain, labTrain):\n",
    "    #     model = featureExtractionModel()\n",
    "\n",
    "    #     optimiser = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    #     model.compile(loss='categorical_crossentropy', metrics=['AUC', 'accuracy'], optimizer=optimiser)\n",
    "\n",
    "    #     print(f'Training for fold {foldN}...')\n",
    "    #     model.fit(imTrain[train], labTrain[train], epochs=4, batch_size=4, validation_data=(imTrain[test], labTrain[test]))\n",
    "\n",
    "    #     foldN += 1\n",
    "    #     score = model.evaluate(imTrain[test], labTrain[test], verbose=0)\n",
    "    #     print(f'Score for fold {foldN}: {model.metrics_names[0]} of {score[0]}; {model.metrics_names[1]} of {score[1]*100}%')\n",
    "\n",
    "    #     if score[0] < bestLoss:\n",
    "    #         bestLoss = score[0]\n",
    "    #         bestModel = model\n",
    "\n",
    "    # bestModel.save(\"./CNN.h5\")\n",
    "\n",
    "\n",
    "    # test_score = model.evaluate(imTest, labTest, verbose=0)\n",
    "    # print(f'Test Score: Loss = {test_score[0]}; AUC = {test_score[1]*100}%; Accuracy = {test_score[2]*100}%')\n",
    "\n",
    "\n",
    "    model = load_model(\"./CNN.h5\")\n",
    "\n",
    "    featureModel = keras.Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
    "\n",
    "    featureTrain = featureModel.predict(imTrain)\n",
    "    print(featureTrain.shape)\n",
    "    featureTest = featureModel.predict(imTest)\n",
    "    print(featureTest.shape)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    randForestDist = { # Grid search takes too long so use Halving Random Search\n",
    "        'n_estimators' : sp_randint(500, 1000),\n",
    "        'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth' : [10, 20, 30, None],\n",
    "        'min_samples_split' : sp_randint(2, 10),\n",
    "        'min_samples_leaf' : sp_randint(1, 8),\n",
    "        'max_features' : ['log2', 'sqrt', None],\n",
    "        'bootstrap' : [True, False],\n",
    "        'warm_start' : [True, False],\n",
    "        'class_weight' : ['balance_subsample', None],\n",
    "    }\n",
    "\n",
    "    #CRAP ==> {'bootstrap': False, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 924, 'warm_start': False} Random Forest Classifier Accuracy: 90.75043630017451%\n",
    "    #n_estimators=1000, random_state=42, max_depth=20, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, bootstrap=False\n",
    "    \n",
    "    #n_estimators=100, random_state=42, max_depth=10, max_features='sqrt', \n",
    "    #                                    min_samples_leaf=10, min_samples_split=20, bootstrap=False\n",
    "    \n",
    "    randForest = RandomForestClassifier(n_estimators=500, random_state=42, max_depth=10, max_features='sqrt', \n",
    "                                        min_samples_leaf=10, min_samples_split=20, bootstrap=False, ccp_alpha=0.03) #ccp_alpha=0.01 made it significantly worse\n",
    "    \n",
    "    crossVal = cross_validate(randForest,featureTrain, np.argmax(labTrain, axis=1), cv=5, return_train_score=True)\n",
    "    print(\"Training score: \", crossVal['train_score'])\n",
    "    print(\"Validation score: \", crossVal['test_score'])\n",
    "    \n",
    "    #forestGridSearch = HalvingRandomSearchCV(estimator=randForest, param_distributions=randForestDist, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "    #forestGridSearch.fit(featureTrain, np.argmax(labTrain, axis=1))\n",
    "    #print(\"Best parameters found: \", forestGridSearch.best_params_)\n",
    "    randForest.fit(featureTrain, np.argmax(labTrain, axis=1))\n",
    "\n",
    "    #optimalRandForest = forestGridSearch.best_estimator_\n",
    "\n",
    "    rfPred = randForest.predict(featureTest)\n",
    "\n",
    "    rfPredProb = randForest.predict_proba(featureTest)\n",
    "    # for i, probas in enumerate(rfPredProb):\n",
    "    #     predicted_class = np.argmax(probas)\n",
    "    #     confidence = np.max(probas)\n",
    "    #     print(f\"Confidence = {confidence}\")\n",
    "\n",
    "\n",
    "    # rfPredRAW = encoder.inverse_transform([np.argmax(label) for label in rfPred])\n",
    "    # realLabelRAW = encoder.inverse_transform([np.argmax(label) for label in labTest])\n",
    "\n",
    "    adjustedPred = []\n",
    "    falseNegHealthy = 0\n",
    "\n",
    "    for i, prob in enumerate(rfPredProb): # The confidences are stored as [a, b, c, d], where each one is a probability between 0 and 1. From them, if class = b and conf of this class is smaller than 0.75, predict the next highest\n",
    "        classPred = np.argmax(prob)\n",
    "        conf = np.max(prob)\n",
    "        if classPred == 1 and conf < 0.6: #0.7 = 1 at 82%, 0.55 = 11 at 85%, 0.65 = 2 at 83.4%, 0.6 = 6 at 84.5. MAX ACCURACY: 22 at 86%.0 \n",
    "            sortConf = np.argsort(prob)[::-1]\n",
    "            newPred = sortConf[1]\n",
    "            adjustedPred.append(newPred)\n",
    "            print(f\"Low confidence ({conf}). Next Prediction: {newPred} with confidence {np.max(newPred)}. Actual: {np.argmax(labTest[i])}\")\n",
    "        else:\n",
    "            adjustedPred.append(classPred)\n",
    "            print(f'Predicted: {classPred}, Confidence: {conf}, Actual: {labTest[i]}')\n",
    "\n",
    "        if adjustedPred[i] != np.argmax(labTest[i]) and adjustedPred[i] == 1: # Count how many false negatives there are.\n",
    "            print()\n",
    "            print(f\"adjustedPred[i] = {adjustedPred[i]}, np.argmax(labTest[i]) = {np.argmax(labTest[i])}\")\n",
    "            falseNegHealthy += 1\n",
    "\n",
    "    print(falseNegHealthy)\n",
    "    countReal = pd.Series(np.argmax(labTest, axis=1)).value_counts()\n",
    "    countPred = pd.Series(adjustedPred).value_counts()\n",
    "\n",
    "    compareCount = pd.DataFrame({'Actual: ': countReal, 'Pred: ': countPred})\n",
    "    print(compareCount)\n",
    "\n",
    "    accuracy = accuracy_score(np.argmax(labTest, axis=1), adjustedPred)\n",
    "    print(f'Random Forest Classifier Accuracy: {accuracy * 100}%')\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "    # hyperparamGrid = {\n",
    "    #     'n_neighbors' : [2, 4, 8],\n",
    "    #     'algorithm' : ['ball_tree', 'brute'],\n",
    "    #     'leaf_size' : [20, 30, 50],\n",
    "    #     'p' : [1, 2, 3],\n",
    "    #     #'metric' : ['auto', 'sqrt']\n",
    "    # }\n",
    "\n",
    "    knnModel = KNeighborsClassifier(n_neighbors=2, weights='uniform', leaf_size=4)\n",
    "    \n",
    "    crossVal = cross_validate(knnModel,featureTrain, np.argmax(labTrain, axis=1), cv=5, return_train_score=True)\n",
    "    print(\"Training score: \", crossVal['train_score'])\n",
    "    print(\"Validation score: \", crossVal['test_score'])\n",
    "    \n",
    "\n",
    "    knnModel.fit(featureTrain, np.argmax(labTrain, axis=1))\n",
    "\n",
    "    knnPredProb = knnModel.predict_proba(featureTest)\n",
    "\n",
    "    adjustedPred = []\n",
    "    falseNegHealthy = 0\n",
    "\n",
    "    for i, prob in enumerate(knnPredProb): # The confidences are stored as [a, b, c, d], where each one is a probability between 0 and 1. From them, if class = b and conf of this class is smaller than 0.75, predict the next highest\n",
    "        classPred = np.argmax(prob)\n",
    "        conf = np.max(prob)\n",
    "        if classPred == 1 and conf < 0.7:\n",
    "            sortConf = np.argsort(prob)[::-1]\n",
    "            newPred = sortConf[1]\n",
    "            adjustedPred.append(newPred)\n",
    "            print(f\"Low confidence ({conf}). Next Prediction: {newPred} with confidence {np.max(newPred)}. Actual: {np.argmax(labTest[i])}\")\n",
    "        else:\n",
    "            adjustedPred.append(classPred)\n",
    "            print(f'Predicted: {classPred}, Confidence: {conf}, Actual: {labTest[i]}')\n",
    "\n",
    "        if classPred != np.argmax(labTest[i]) and np.argmax(labTest[i]) == 1: # Count how many false negatives there are.\n",
    "            print()\n",
    "            print(f\"classPred = {classPred}, np.argmax(labTest[i]) = {np.argmax(labTest[i])}\")\n",
    "            falseNegHealthy += 1\n",
    "\n",
    "    print(falseNegHealthy)\n",
    "    countReal = pd.Series(np.argmax(labTest, axis=1)).value_counts()\n",
    "    countPred = pd.Series(adjustedPred).value_counts()\n",
    "\n",
    "    compareCount = pd.DataFrame({'Actual: ': countReal, 'Pred: ': countPred})\n",
    "    print(compareCount)\n",
    "\n",
    "    accuracy = accuracy_score(np.argmax(labTest, axis=1), adjustedPred)\n",
    "    print(f'KNN Accuracy: {accuracy * 100}%')\n",
    "\n",
    "'''\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
